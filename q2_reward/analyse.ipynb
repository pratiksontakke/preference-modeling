{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Reward Model Training and Analysis\n",
        "\n",
        "This notebook implements a reward model to learn preferences from ranked responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
        "from trl import RewardTrainer\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "MAX_LENGTH = 512\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "df = pd.read_csv('answers.csv')\n",
        "print(f\"Loaded {len(df)} examples\")\n",
        "\n",
        "# Create pairs of responses for comparison\n",
        "def create_pairs(group):\n",
        "    pairs = []\n",
        "    for i in range(len(group)):\n",
        "        for j in range(i + 1, len(group)):\n",
        "            if group.iloc[i]['rank'] < group.iloc[j]['rank']:  # i is better than j\n",
        "                pairs.append({\n",
        "                    'prompt': group.iloc[i]['prompt'],\n",
        "                    'better': group.iloc[i]['answer'],\n",
        "                    'worse': group.iloc[j]['answer']\n",
        "                })\n",
        "            elif group.iloc[i]['rank'] > group.iloc[j]['rank']:  # j is better than i\n",
        "                pairs.append({\n",
        "                    'prompt': group.iloc[i]['prompt'],\n",
        "                    'better': group.iloc[j]['answer'],\n",
        "                    'worse': group.iloc[i]['answer']\n",
        "                })\n",
        "    return pairs\n",
        "\n",
        "# Create training pairs\n",
        "all_pairs = []\n",
        "for prompt in df['prompt'].unique():\n",
        "    prompt_group = df[df['prompt'] == prompt]\n",
        "    pairs = create_pairs(prompt_group)\n",
        "    all_pairs.extend(pairs)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(all_pairs))\n",
        "print(f\"Created {len(train_dataset)} training pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=1,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"reward_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from trl import RewardTrainer\n",
        "import numpy as np\n",
        "from datasets import Dataset\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
